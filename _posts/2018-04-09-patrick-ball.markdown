---
layout: post
title:  "Data Science Perspectives w/ Dr. Patrick Ball"
date:   2018-04-09 12:00:00
categories: data-science
comments: true
---

> “When we use bad data thoughtlessly, we will reinforce our prejudices, not arrest them.” 
> - Dr. Patrick Ball

Questioning first principles isn't always the most natural thing for the data scientist (or any researcher, for that matter). First principles can be subjective and messy, not to mention intractable, which is not the type of answer people want to hear from highly paid scientists. Deep neural networks should be able to solve anything!

This tension was at the center of the keynote talk [Dr. Patrick Ball](https://en.wikipedia.org/wiki/Patrick_Ball) gave for the Vanderbilt Data Science Symposium on March 30. Dr. Ball is the executive director of Human Rights Data Analysis Group, and he assists human rights defenders by conducting rigorous scientific and statistical analysis of large-scale human rights abuses - e.g. in El Salvador, Haiti and Peru. The entire talk was riveting: below are some of his key points.

> The question is not the model. It’s the data.

Take, for instance, the idea of predictive policing. "We are not using *past crime* to predict *future crime*. We are using *past police activity* (the data available) to predict *future police activity* (the performance metric)... So the problem is not in ‘biased features’. It's that we are using patterns in police records to predict what will be detected by police.”

> In human rights, victim’s voices always come first. Statistics are a footnote – but they must be true.

Statistics are a way of knowing the world, of ordering phenomena in such a way that patterns become apparent. They are *evidence* not the case. So they must be treated as the supporting cast, meaning they must support the point! 

But statistics are limited: there are only three ways for your data to make a valid statistical point: 
1) it's a perfect census of the population
2) it's a random sample
3) you accommodate biases with posterior modeling

"You have to assume the data is not all true. You have to determine, how sensitive is your data / model to lies?" For example, in data recording people lost in war, 99% of large events (10+ people killed)  get reported, often by multiple news outlets. Less than 75% of small events get reported, though: meaning the death toll can be much higher than expected - or, in the case of Afghanistan, a largely unreported ethnic cleansing was under way but was masked by the larger war.

> As statisticians, we MUST face the community impact.

We do not operate in a vacuum. And many of the *solutions* offered by advanced analytics create a barrier to uneducated folks participating meaningfully in the conversation. Take corruption in the legal system: "We know how to fight a bad judge, and many people can participate in that fight. But once we have a black box [assigning bail / probation, for example], we’re stuck with it.  For a black box, it becomes a rarefied debate that only academics can participate in." 

As data scientists, we cannot forget that many people don’t know what an algorithm is. It's nothing to be ashamed of -- I didn't fully understand what an algorithm was until my PhD. But as we promote our solutions to common problems, as we begin to "replace" human jobs with AI, we have to be sensitive to the community.

> When people follow the math, they can remember it.

To repeat that last point more optimistically: statistics is compelling because it *is* powerful, because it does reveal meaningful trends that might not otherwise be apparent. And people recognize that: “When judges *want* to know the answer, they do just fine." Let's conclude on that point: that data scientists must partner with the community and communicate effectively, or they risk being problem-makers, rather than problem-solvers.


